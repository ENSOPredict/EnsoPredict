{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNRM_Logistic.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYxtvnmw8vC2"
      },
      "source": [
        "%matplotlib inline\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.ensemble\n",
        "import scipy.stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.ensemble\n",
        "import scipy.stats\n",
        "from sklearn.model_selection import train_test_split \n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVCqrjK68w0g"
      },
      "source": [
        "#Scaffold code to load in data.  This code cell is mostly data wrangling\n",
        "\n",
        "\n",
        "def load_enso_indices():\n",
        "  \"\"\"\n",
        "  Reads in the txt data file to output a pandas Series of ENSO vals\n",
        "\n",
        "  outputs\n",
        "  -------\n",
        "\n",
        "    pd.Series : monthly ENSO values starting from 1870-01-01\n",
        "  \"\"\"\n",
        "  with open('nino34.long.anom.data.txt') as f:\n",
        "    line = f.readline()\n",
        "    enso_vals = []\n",
        "    while line:\n",
        "        yearly_enso_vals = map(float, line.split()[1:])\n",
        "        enso_vals.extend(yearly_enso_vals)\n",
        "        line = f.readline()\n",
        "\n",
        "  enso_vals = pd.Series(enso_vals)\n",
        "  enso_vals.index = pd.date_range('1870-01-01',freq='MS',\n",
        "                                  periods=len(enso_vals))\n",
        "  enso_vals.index = pd.to_datetime(enso_vals.index)\n",
        "  return enso_vals\n",
        "\n",
        "def assemble_predictors_predictands(start_date, end_date, lead_time, \n",
        "                                    dataset, data_format,\n",
        "                                    num_input_time_steps=1,\n",
        "                                    use_pca=False, n_components=32,\n",
        "                                    lat_slice=None, lon_slice=None):\n",
        "  \"\"\"\n",
        "  inputs\n",
        "  ------\n",
        "\n",
        "      start_date           str : the start date from which to extract sst\n",
        "      end_date             str : the end date \n",
        "      lead_time            str : the number of months between each sst\n",
        "                              value and the target Nino3.4 Index\n",
        "      dataset              str : 'observations' 'CNRM' or 'MPI'\n",
        "      data_format          str : 'spatial' or 'flatten'. 'spatial' preserves\n",
        "                                  the lat/lon dimensions and returns an \n",
        "                                  array of shape (num_samples, num_input_time_steps,\n",
        "                                  lat, lon).  'flatten' returns an array of shape\n",
        "                                  (num_samples, num_input_time_steps*lat*lon)\n",
        "      num_input_time_steps int : the number of time steps to use for each \n",
        "                                 predictor sample\n",
        "      use_pca             bool : whether or not to apply principal components\n",
        "                              analysis to the sst field\n",
        "      n_components         int : the number of components to use for PCA\n",
        "      lat_slice           slice: the slice of latitudes to use \n",
        "      lon_slice           slice: the slice of longitudes to use\n",
        "\n",
        "  outputs\n",
        "  -------\n",
        "      Returns a tuple of the predictors (np array of sst temperature anomalies) \n",
        "      and the predictands (np array the ENSO index at the specified lead time).\n",
        "\n",
        "  \"\"\"\n",
        "  file_name = {'observations' : 'sst.mon.mean.trefadj.anom.1880to2018.nc',\n",
        "               'observations2': 'regridded_era_t2m_anomalies.nc',\n",
        "               'CNRM'         : 'CNRM_tas_anomalies_regridded.nc',\n",
        "               'MPI'          : 'MPI_tas_anomalies_regridded.nc'}[dataset]\n",
        "  variable_name = {'observations' : 'sst',\n",
        "                   'observations2': 't2m',\n",
        "                   'CNRM'         : 'tas',\n",
        "                   'MPI'          : 'tas'}[dataset]\n",
        "  ds = xr.open_dataset(file_name)\n",
        "  sst = ds[variable_name].sel(time=slice(start_date, end_date))\n",
        "  if lat_slice is not None:\n",
        "    try:\n",
        "        sst=sst.sel(lat=lat_slice)\n",
        "    except:\n",
        "        raise NotImplementedError(\"Implement slicing!\")\n",
        "  if lon_slice is not None:\n",
        "    try:\n",
        "        sst=sst.sel(lon=lon_slice)\n",
        "    except:\n",
        "        raise NotImplementedError(\"Implement slicing!\")\n",
        "  \n",
        "  \n",
        "  num_samples = sst.shape[0]\n",
        "  #sst is a (num_samples, lat, lon) array\n",
        "  #the line below converts it to (num_samples, num_input_time_steps, lat, lon)\n",
        "  sst = np.stack([sst.values[n-num_input_time_steps:n] for n in range(num_input_time_steps,\n",
        "                                                              num_samples+1)])\n",
        "  #CHALLENGE: CAN YOU IMPLEMENT THE ABOVE LINE WITHOUT A FOR LOOP?\n",
        "  num_samples = sst.shape[0]\n",
        "\n",
        "  sst[np.isnan(sst)] = 0\n",
        "  if data_format=='flatten':\n",
        "    #sst is a 3D array: (time_steps, lat, lon)\n",
        "    #in this tutorial, we will not be using ML models that take\n",
        "    #advantage of the spatial nature of global temperature\n",
        "    #therefore, we reshape sst into a 2D array: (time_steps, lat*lon)\n",
        "    #(At each time step, there are lat*lon predictors)\n",
        "    sst = sst.reshape(num_samples, -1)\n",
        "    \n",
        "\n",
        "    #Use Principal Components Analysis, also called\n",
        "    #Empirical Orthogonal Functions, to reduce the\n",
        "    #dimensionality of the array\n",
        "    if use_pca:\n",
        "      pca = sklearn.decomposition.PCA(n_components=n_components)\n",
        "      pca.fit(sst)\n",
        "      X = pca.transform(sst)\n",
        "    else:\n",
        "      X = sst\n",
        "  else: # data_format=='spatial'\n",
        "    X = sst\n",
        "\n",
        "  start_date_plus_lead = pd.to_datetime(start_date) + \\\n",
        "                        pd.DateOffset(months=lead_time+num_input_time_steps-1)\n",
        "  end_date_plus_lead = pd.to_datetime(end_date) + \\\n",
        "                      pd.DateOffset(months=lead_time)\n",
        "  if dataset == 'observations':\n",
        "    y = load_enso_indices()[slice(start_date_plus_lead, \n",
        "                                  end_date_plus_lead)]\n",
        "  else: #the data is from a GCM\n",
        "    X = X.astype(np.float32)\n",
        "    #The Nino3.4 Index is composed of three month rolling values\n",
        "    #Therefore, when calculating the Nino3.4 Index in a GCM\n",
        "    #we have to extract the two months prior to the first target start date\n",
        "    target_start_date_with_2_month = start_date_plus_lead - pd.DateOffset(months=2)\n",
        "    subsetted_ds = ds[variable_name].sel(time=slice(target_start_date_with_2_month,\n",
        "                                                   end_date_plus_lead))\n",
        "    #Calculate the Nino3.4 index\n",
        "    y = subsetted_ds.sel(lat=slice(5,-5), lon=slice(360-170,360-120)).mean(dim=('lat','lon'))\n",
        "\n",
        "    y = pd.Series(y.values).rolling(window=3).mean()[2:].values\n",
        "    y = y.astype(np.float32)\n",
        "  ds.close()\n",
        "  return X.astype(np.float32), y.astype(np.float32)\n",
        "\n",
        "\n",
        "class ENSODataset(Dataset):\n",
        "    def __init__(self, predictors, predictands):\n",
        "        self.predictors = predictors\n",
        "        self.predictands = predictands\n",
        "        assert self.predictors.shape[0] == self.predictands.shape[0], \\\n",
        "               \"The number of predictors must equal the number of predictands!\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.predictors.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.predictors[idx], self.predictands[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUIEcduA8zkz"
      },
      "source": [
        "train_start_date = '1860-01-01'\n",
        "train_end_date = '2200-12-31'\n",
        "num_input_time_steps = 2\n",
        "lead_time = 2 # 1,2,3,4,... 12\n",
        "#climate_model = 'MPI'  # CNRM\n",
        "climate_model = 'CNRM'\n",
        "train_predictors, train_predictands = assemble_predictors_predictands(train_start_date,\n",
        "                      train_end_date, lead_time, climate_model, 'flatten', num_input_time_steps=num_input_time_steps)\n",
        "test_predictors, test_predictands = assemble_predictors_predictands('1981-01-01',\n",
        "                    '2010-12-31', lead_time, 'observations', 'flatten', num_input_time_steps=num_input_time_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMKfZ2v_8-3w"
      },
      "source": [
        "def toClassification(y):\n",
        "\n",
        "  i=0\n",
        "  for value in y:\n",
        "    \n",
        "    if value<=-1.5:\n",
        "      y[i]=-1\n",
        "      i=i+1\n",
        "    elif value>=1.5:\n",
        "      y[i]=1\n",
        "      i=i+1\n",
        "    else:\n",
        "      y[i]=0\n",
        "      i=i+1\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqCP7GOg9Au3"
      },
      "source": [
        "X_train=train_predictors\n",
        "y_train=toClassification(train_predictands)\n",
        "X_val=test_predictors\n",
        "y_val=toClassification(test_predictands)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_UAR2YB9Cbl"
      },
      "source": [
        "print(\"Number of La Nina:\",np.count_nonzero(y_train==-1))\n",
        "print(\"Number of Neutral State:\",np.count_nonzero(y_train==0))\n",
        "print(\"Number of El Nino:\",np.count_nonzero(y_train==1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB44-wuX9Dr1"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "logreg = LogisticRegression(C=1e5)\n",
        "logreg.fit(X_train,y_train)\n",
        "predictions = logreg.predict(X_val)\n",
        "confusion_matrix(y_val, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzM073-29FO3"
      },
      "source": [
        "cm = confusion_matrix(y_val, predictions)\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.imshow(cm)\n",
        "ax.grid(False)\n",
        "ax.xaxis.set(ticks=(0, 1,2), ticklabels=('Predicted -1s','Predicted 0s', 'Predicted 1s'))\n",
        "ax.yaxis.set(ticks=(0, 1,2), ticklabels=('Actual -1s','Actual 0s', 'Actual 1s'))\n",
        "ax.set_ylim(2.5, -0.5)\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcwKvv9L9IRw"
      },
      "source": [
        "print(classification_report(y_val, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-AHuy0A9IT9"
      },
      "source": [
        "def plot_nino_time_series(y, predictions, title):\n",
        "  \"\"\"\n",
        "  inputs\n",
        "  ------\n",
        "    y           pd.Series : time series of the true Nino index\n",
        "    predictions np.array  : time series of the predicted Nino index (same\n",
        "                            length and time as y)\n",
        "    titile                : the title of the plot\n",
        "\n",
        "  outputs\n",
        "  -------\n",
        "    None.  Displays the plot\n",
        "  \"\"\"\n",
        "  predictions = pd.Series(predictions, index=y.index)\n",
        "  predictions = predictions.sort_index()\n",
        "  y = y.sort_index()\n",
        "\n",
        "  plt.plot(y, label='Ground Truth')\n",
        "  plt.plot(predictions, '--', label='ML Predictions')\n",
        "  plt.legend(loc='best')\n",
        "  plt.title(title)\n",
        "  plt.ylabel('ONI Index')\n",
        "  plt.xlabel('Date')\n",
        "  plt.show()\n",
        "  plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE1kQyAY9LMJ"
      },
      "source": [
        "experiment_name=\"Logistic Regression_{}_{}_lead_time{}\".format(train_start_date, train_end_date, str(lead_time))\n",
        "corr, _ = pearsonr(y_val, predictions)\n",
        "rmse = mean_squared_error(y_val, predictions) ** 0.5\n",
        "plot_nino_time_series(y_val, predictions, '{} Predictions. Corr: {:3f}. RMSE: {:3f}.'.format(experiment_name,\n",
        "                                                                      corr, rmse))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}